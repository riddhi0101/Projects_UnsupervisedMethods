{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 2. Clustering using k-means\n",
    "\n",
    "### Student ID: 916063788\n",
    "### Student Name: Riddhi Barbhaiya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named basemap",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f5ace32acb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasemap\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasemap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpylab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrcParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named basemap"
     ]
    }
   ],
   "source": [
    "# All Import Statements Defined Here\n",
    "# Note: Do not change anything\n",
    "\n",
    "import numpy\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Do not use any packages below in your code before part 4\n",
    "\n",
    "import pandas as pd\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "from pylab import rcParams\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sklearn # use sklearn to do clustering only if you decide not to get credit from part 1~3\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "There are two sections in this project: \n",
    "- 1. write a python class to implement the k-means method and its variant, along with a clustering evaluation metric. \n",
    "- 2. use the python class you write to do some data analysis.\n",
    "\n",
    "Note, you can do the majority of section 2 without finishing section 1 by using functions in sklearn, but this automatically leads to zero credit for part 1-3.\n",
    "\n",
    "This project is very challenging, start early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Implementing k-means algorithm <font color=\"red\">  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete what is missing to implement the k means algorithm. Read the explicit requirements inside the functions carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class k_means:\n",
    "    \n",
    "    def __init__(self, data: numpy.ndarray, d: int, k: int , tol: float, max_iter: int, seed = 20):\n",
    "        \"\"\"\n",
    "        data: data to cluster\n",
    "        d:dimension of the data\n",
    "        k: prespecified number of clusters\n",
    "        tol: convergence criterion\n",
    "        max_iter: maximum number of iterations allowed\n",
    "        \"\"\"\n",
    "        ### self.partition[i] stores the indices of the points belongs to cluster i\n",
    "        ### self.labels[i] is the label (from 0-k) for data point i\n",
    "        \n",
    "        self.partitions={i:[] for i in range(k) }\n",
    "        self.centers = np.zeros((k,d))\n",
    "        self.next_centers= np.zeros((k,d))\n",
    "        self.labels=[]\n",
    "        self.d=d\n",
    "        self.n=data.shape[0]\n",
    "        self.counter=0\n",
    "        self.seed = seed\n",
    "        \n",
    "        ### number of lines: <= 5\n",
    "        ### initialize all the necessary variables to store the parameters used in the algorithm\n",
    "        ### \n",
    "        ### your code starts here\n",
    "\n",
    "        self.data = data\n",
    "        self.k, self.max_iter, self.tol = k, max_iter, tol\n",
    "        self.labels = np.zeros(self.n)\n",
    "    \n",
    "        ### end of your code\n",
    "    \n",
    "    \n",
    "    def initilize_centers(self ,method: int):\n",
    "        \"\"\"\n",
    "        method = 0:\n",
    "        pick the first k points from the data as centers\n",
    "\n",
    "        method = 1:\n",
    "        randomly pick k points from the data as centers\n",
    "        \"\"\"\n",
    "        if method==0:\n",
    "            self.centers=self.data[:self.k,:]\n",
    "    \n",
    "        elif method==1:\n",
    "            np.random.seed(20)\n",
    "        ### number of lines: = 1\n",
    "        ### use nump.random.choice to initialize the centers;\n",
    "        \n",
    "        ### your code starts here\n",
    "        \n",
    "           self.centers = [self.data[i] for i in np.random.choice(self.n, self.k, replace=False)]\n",
    "            \n",
    "        ### end of your code\n",
    "        \n",
    "\n",
    "    def search(self):\n",
    "        \"\"\"\n",
    "        update the patitions and then calculate the next centers; \n",
    "        here we use centroids for k-means method\n",
    "        \"\"\"\n",
    "        self.partitions={i:[] for i in range(self.k)}\n",
    "        self.next_centers=np.array([])\n",
    "\n",
    "        ### number of lines: <= 10\n",
    "        ### update the self.partitions based on current self.centers;\n",
    "        ### update the self.next_centers based on self.partitions;\n",
    "        \n",
    "        ### your code starts here\n",
    "        \n",
    "        self.next_centers = np.zeros((self.k, self.d))\n",
    "        for dataPointind in range(self.n):\n",
    "            point = self.data[dataPointind]\n",
    "            distances = np.array([np.linalg.norm(point - center) for center in self.centers])\n",
    "            minDistInd = np.argmin(distances)\n",
    "            self.partitions[minDistInd].append(point)\n",
    "            self.labels[dataPointind] = minDistInd\n",
    "        # calculating next centers\n",
    "        for i, dataPoints in self.partitions.items():\n",
    "            self.next_centers[i] = np.mean(np.array(dataPoints), axis=0)\n",
    "\n",
    "        \n",
    "        ### end of your code\n",
    "            \n",
    "            \n",
    "    def is_updated(self):\n",
    "        \"\"\"\n",
    "        return True if update is completed, namely, the algorithm has not yet converged; \n",
    "        return False otherwise;\n",
    "        The convergence criterion is the sum of absolute relative differences between \n",
    "        self.centers and self.next_centers smaller than tol;\n",
    "        \"\"\"\n",
    "        ### number of lines: <= 6\n",
    "        ### your code starts here\n",
    "        \n",
    "        crit = np.sum(np.absolute((self.centers - self.next_centers) / self.centers))\n",
    "        if crit < self.tol:\n",
    "            return False\n",
    "        return True\n",
    "        \n",
    "        ### end of your code\n",
    "\n",
    "        \n",
    "    def fit_model(self):\n",
    "        \"\"\"\n",
    "        function to fit the k-means algorithms using the above functions\n",
    "        \"\"\"\n",
    "        self.initilize_centers(1)\n",
    "        self.counter = 0\n",
    "        \n",
    "        ### number of lines: <= 15\n",
    "        ### come up with a control flow to use self.search and self.is_updated to finish the model fitting;\n",
    "        ### print \"Maximum Number of Iteration Reached!\" if the number of iterations = the maximum iterations allowed;\n",
    "        ### print \"Convergence Reached! Number of Iterations: __\" when the fitting is successfully done\n",
    "        ### \n",
    "        ### your code starts here\n",
    "\n",
    "        status = self.is_updated()\n",
    "        while status:\n",
    "            self.search()\n",
    "            if self.counter == self.max_iter:\n",
    "                print(\"Maximum Number of Iteration Reached!\")\n",
    "                break\n",
    "            self.counter += 1\n",
    "            status = self.is_updated()\n",
    "            self.centers = np.copy(self.next_centers)\n",
    "\n",
    "        if not status:\n",
    "            print(\"Convergence Reached! Number of Iterations: \", self.counter)\n",
    "\n",
    "        ### your code ends here\n",
    "    \n",
    "        self.get_labels()\n",
    "\n",
    "        \n",
    "    def set_k(self,k):\n",
    "        self.k=k\n",
    "        \n",
    "    def predict(self,pt):\n",
    "        distances = [ numpy.linalg.norm( pt-c ) for c in self.centers ]\n",
    "        cluster_label = distances.index( min(distances) )\n",
    "        return cluster_label\n",
    "\n",
    "    \n",
    "    def get_labels(self):\n",
    "        \n",
    "        ### number of lines: <= 6\n",
    "        ### write an api to update self.labels based on self.partitions\n",
    "        ### your code starts here\n",
    "        \n",
    "        # done in search\n",
    "        \n",
    "        ### end of your code\n",
    "        \n",
    "        return self.labels\n",
    "    \n",
    "    def get_centers(self):\n",
    "        return self.centers\n",
    "    \n",
    "    def get_clusters(self):\n",
    "        return self.partitions\n",
    "\n",
    "    def get_cost(self):\n",
    "        \"\"\"\n",
    "        Here we use within cluster sum of squares as cost \n",
    "        \"\"\"\n",
    "        \n",
    "        ### number of lines: <= 6\n",
    "        ### write an api to get the cost based on self.partition and self.next_centers (why?)\n",
    "        ### your code starts here\n",
    "        kWithinClustSS = []\n",
    "        for i, dataPoints in self.partitions.items():\n",
    "            kWithinClustSS.append(sum([(np.linalg.norm(j - self.next_centers[i]) ** 2) for j in dataPoints]))\n",
    "        self.cost = sum(kWithinClustSS)\n",
    "        \n",
    "    \n",
    "        \n",
    "        ### end of your code\n",
    "        return self.cost\n",
    "        \n",
    "    def plot_clusters(self):\n",
    "        if self.d>2:\n",
    "            print(\"Dimension too large!\")\n",
    "            return \n",
    "        if self.labels==[]:\n",
    "            self.fit_model()\n",
    "        plt.scatter( self.data[:,0] , self.data[:,1], c=self.labels ,s=3)\n",
    "        plt.scatter( np.array(self.centers)[:,0],np.array(self.centers)[:,1] ,marker='*',c=list(range(self.k)) ,s=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Implementing criteria to evaluate clustering algorithms <font color=\"red\"> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class to implement the clustering evaluation metric \"adjusted Rand score\". To leearn about the adjusted Rand score watch the video guide or read more details that can be found here https://en.wikipedia.org/wiki/Rand_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class clustering_eval_metrics:\n",
    "    def __init__(self, labels: list ,true_labels=None): # label must be between 0 to number_of_labels - 1\n",
    "        \"\"\"\n",
    "        labels: list of labels that are from the clustering algorithm, for example, from the get_labels method in\n",
    "                the above k_means class\n",
    "        true_labels: the ground truth of the labels\n",
    "        \"\"\"\n",
    "        self.labels=numpy.array(labels)\n",
    "        self.true_labels=true_labels\n",
    "        self.cmat=None\n",
    "        self.ars=None\n",
    "        \n",
    "    def set_true_labels(self, true_labels):\n",
    "        self.true_labels=numpy.array(true_labels)\n",
    "    \n",
    "    def contingency_matrix(self): \n",
    "        \"\"\"\n",
    "        return a contingency matrix\n",
    "        \"\"\"\n",
    "        ### your code starts here\n",
    "        \n",
    "        k = len(np.unique(self.labels))\n",
    "        self.cmat = np.zeros((k, k))\n",
    "        self.labels = self.labels.astype(int)\n",
    "        self.true_labels = self.true_labels.astype(int)\n",
    "        for i, j in zip(self.labels, self.true_labels):\n",
    "            self.cmat[i][j] += 1\n",
    "        \n",
    "        ### end of your code\n",
    "        \n",
    "        return self.cmat\n",
    "        \n",
    "    def adjusted_rand_score(self):\n",
    "        \"\"\"\n",
    "        return the adjusted_rand_score\n",
    "        \"\"\"\n",
    "        ### your code starts here\n",
    "        \n",
    "        choose2 = lambda a: a * (a - 1) / 2\n",
    "        self.contingency_matrix()\n",
    "        summedRows = np.sum(self.cmat, axis=0)\n",
    "        summedCols = np.sum(self.cmat, axis=1)\n",
    "        sumbjc2 = sum([choose2(j) for j in summedRows])\n",
    "        sumaic2 = sum([choose2(i) for i in summedCols])\n",
    "        sumNijc2 = sum([choose2(ij) for ij in self.cmat.flatten()])\n",
    "        nC2 = choose2(len(self.labels))\n",
    "        num = sumNijc2 - ((sumaic2 * sumbjc2) / nC2)\n",
    "        den = (0.5 * (sumaic2 + sumbjc2)) - ((sumaic2 * sumbjc2) / nC2)\n",
    "        self.ars = num / den\n",
    "        \n",
    "        ### end of your code\n",
    "\n",
    "        return self.ars\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. k-medoid algorithm <font color=\"red\">  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a class called pam to implement the k-medoid algorithm. It should have a similar structure as the k_means class as we implemented before. Write the code as concisely as possible. Any code that exceeds 40 lines will get penalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class pam(k_means):\n",
    "    def __init__(self, data: numpy.ndarray, d: int, k: int , tol: float, max_iter: int, p: float):\n",
    "        \"\"\"\n",
    "        p is the parameter for the L_p norm\n",
    "        \"\"\"\n",
    "    ### hint: think about the differences between pam and k_means\n",
    "    ###       in fact, there are not many, right?\n",
    "    ### Your code starts here\n",
    "    \n",
    "        super().__init__(data, d, k, tol, max_iter)\n",
    "        self.p = p\n",
    "\n",
    "    def search(self):\n",
    "        self.next_centers = np.zeros((self.k, self.d))\n",
    "        self.partitions = {i: [] for i in range(self.k)}\n",
    "        pnorm_dist = lambda a, b: np.linalg.norm(b - a, ord=self.p)\n",
    "\n",
    "        for dataPointind in range(self.n):\n",
    "            point = self.data[dataPointind]\n",
    "            distances = np.array([pnorm_dist(point, center) for center in self.centers])\n",
    "            minDistInd = np.argmin(distances)\n",
    "            self.partitions[minDistInd].append(point)\n",
    "            self.labels[dataPointind] = minDistInd\n",
    "        # calculating next centers\n",
    "        for i, dataPoints in self.partitions.items():\n",
    "            if dataPoints != []:\n",
    "                minInd = np.argmin([sum([pnorm_dist(point,l) for l in dataPoints]) for point in dataPoints])\n",
    "                self.next_centers[i] = dataPoints[minInd]\n",
    "\n",
    "    def predict(self,pt):\n",
    "        distances = [ numpy.linalg.norm( pt-c, ord=self.p) for c in self.centers ]\n",
    "        cluster_label = distances.index( min(distances) )\n",
    "        return cluster_label\n",
    "    \n",
    "    ### your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Simulation Study  <font color=\"red\">  </font>\n",
    "\n",
    "### You may choose not to use the functions written above to finish this part. Then, you automatically lose all the points from Part 1~3.\n",
    "\n",
    "Sample $60$ data points each from the following distributions each\n",
    "\n",
    "$$ X_1\\sim N\\bigg(\\begin{pmatrix}\n",
    "0\\\\\n",
    "0\\end{pmatrix},\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1 \\end{pmatrix}\\bigg),X_2\\sim N\\bigg(\\begin{pmatrix}\n",
    "3\\\\\n",
    "2\\end{pmatrix},\\begin{pmatrix}\n",
    "2 & 1\\\\\n",
    "1 & 1 \\end{pmatrix}\\bigg)\n",
    ", X_3\\sim N\\bigg(\\begin{pmatrix}\n",
    "5\\\\\n",
    "0\\end{pmatrix},\\begin{pmatrix}\n",
    "2 & 1\\\\\n",
    "1 & 1 \\end{pmatrix}\\bigg) $$\n",
    "\n",
    "to then concantenate these points sequentially to form a sample of size $180$.  Use numpy.random.multivariate_normal() and set numpy.random.seed(20) right before you call numpy.random.multivariate_normal() each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.zeros((180,2))\n",
    "true_labels=np.zeros(180)\n",
    "\n",
    "### store your simulated data in data;\n",
    "### store the labels 0-2 in true_labels;\n",
    "\n",
    "### your code starts here\n",
    "\n",
    "center1 = [0,0]\n",
    "cov1 = [[1,0],[0,1]]\n",
    "numpy.random.seed(20)\n",
    "data[:60] = np.random.multivariate_normal(center1,cov1,60)\n",
    "true_labels[:60] = np.zeros(60)\n",
    "\n",
    "center2 = [3,2]\n",
    "cov2 = [[2,1],[1,1]]\n",
    "np.random.seed(20)\n",
    "data[60:120] = np.random.multivariate_normal(center2,cov2,60)\n",
    "true_labels[60:120] = np.ones(60)\n",
    "\n",
    "\n",
    "center3 = [5,0]\n",
    "cov3 = [[2,1],[1,1]]\n",
    "np.random.seed(20)\n",
    "data[120:] = np.random.multivariate_normal(center3,cov3,60)\n",
    "true_labels[120:] = np.ones(60)*2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### your code ends here\n",
    "\n",
    "np.random.seed(20)\n",
    "ind = np.arange(180)\n",
    "np.random.shuffle(ind)\n",
    "data = data[ind]\n",
    "true_labels = true_labels[ind]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"Red\"> Run the following code.</font> (do not change the code!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_data = np.genfromtxt('./ref_data/part4_sim.csv', delimiter=',')\n",
    "ref_labels = np.genfromtxt('./ref_data/part4_sim_labels.csv', delimiter=',').astype(np.int32)\n",
    "ref_cluster = np.genfromtxt('./ref_data/part4_sim_clusters.csv', delimiter=',')\n",
    "ref_ari_score = np.genfromtxt('./ref_data/part4_sim_ari.csv', delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following lines are tests to see whether you simulated data correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(true_labels,ref_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_almost_equal(data,ref_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Apply k-means (with k=3, d=2, tol = 1e-7, max_iter = 500) to the simulated data set. Plot different clusters and their centers, use the plot_clusters() API in the k_means class. Also calculate the adjusted rand score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ref_data\n",
    "true_labels = ref_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### default value\n",
    "\n",
    "km = None\n",
    "ari_score = None\n",
    "\n",
    "### km is the k_means class object that you create using data;\n",
    "### ari_score is the ari score of the predicted labels;\n",
    "\n",
    "### your code starts here\n",
    "\n",
    "km = k_means(data, d=2, k=3, tol=1e-7, max_iter=500)\n",
    "print(\"k_means\")\n",
    "km.fit_model()\n",
    "km.plot_clusters()\n",
    "labels = km.get_labels()\n",
    "eval = clustering_eval_metrics(labels, true_labels)\n",
    "ari_score = eval.adjusted_rand_score()\n",
    "\n",
    "### your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"Red\"> Run the following code.</font> (do not change the code!)\n",
    "\n",
    "### The following lines are tests to see whether you implement the methods correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_array_equal(km.get_labels(),ref_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert abs(ari_score - ref_ari_score[0]) < 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2a Apply pam method (set k=3, d=2, tol = 1e-7, max_iter = 500) to the simulated data set. Plot different clusters and their centers using the L_p \"norm\" when p=.1 and p=2. Also calculate the adjusted rand score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For p = .1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### your code starts here\n",
    "\n",
    "pam01 = pam(data, d=2, k=3, tol=1e-7, max_iter=500, p=0.1)\n",
    "print(\"pam p = 0.1\")\n",
    "pam01.fit_model()\n",
    "pam01.plot_clusters()\n",
    "labels = pam01.get_labels()\n",
    "eval = clustering_eval_metrics(labels, true_labels)\n",
    "ari_score = eval.adjusted_rand_score()\n",
    "\n",
    "### your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For p = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### your code starts here\n",
    "\n",
    "pam2 = pam(data, d=2, k=3, tol=1e-7, max_iter=500, p=2)\n",
    "print(\"pam p=2\")\n",
    "pam2.fit_model()\n",
    "pam2.plot_clusters()\n",
    "labels = pam2.get_labels()\n",
    "eval = clustering_eval_metrics(labels, true_labels)\n",
    "ari_score = eval.adjusted_rand_score()\n",
    "\n",
    "### your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2b Can you compare these two results and analyze quantitatively the cause of the difference?\n",
    "\n",
    "Hint: You can make a plot of the L_p distance function to see the behavior when p= .1 and p=2 to reason why they behave dramatically different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(ncols=2,nrows=1,sharex=True, sharey=True)\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    "p01 = 0.1\n",
    "x = np.arange(-5,6)\n",
    "y = np.arange(-5,6)\n",
    "xx,yy = np.meshgrid(x,y)\n",
    "zz1 = ((np.abs((xx)) ** p01) + (np.abs((yy)) ** p01)) ** (1. / p01)\n",
    "a = ax[0].contourf(xx, yy, zz1, 30, cmap='bwr')\n",
    "ax[0].title.set_text('p = 0.1')\n",
    "zz2 = ((np.abs((xx)) ** 2) + (np.abs((yy)) ** 2)) ** (1. / 2)\n",
    "ax[1].contourf(xx, yy, zz2, 30, cmap='bwr')\n",
    "ax[1].title.set_text('p = 2')\n",
    "fig.colorbar(a)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points that lie on an axis are really close to each other by L_p norm when p=0.1. This is why in the clustering, some points that look faraway from the rest of the points are mapped to the same cluser. The p=0.1 distance will deem them to be close if they have one of the cordinates is close. On the other hand, the 2 norm will look at both coordinates to calculate distance and represents what we intuitively think of as distanc between points. The use of these two different distances results in the different clusterings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2c Are the results from k_means and pam the same when p=2? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"diff between km and pam2 labels\")\n",
    "print(km.get_labels() - pam2.get_labels())\n",
    "print(\"diff between km and pam2 centers\")\n",
    "print(km.get_centers() - pam2.get_centers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are not the same because the centers that each algorithm finds is different. In contrast to pam = 2, centers in k_means do not have to be data points. Although both use the L2 norm, pam is further restricted in the choice of centers resulting in a different clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 How to choose k? First interpret the plot that you get from the code below, then come up with a procedure using this plot to find a k. What k you would like to use? Explain why. \n",
    "\n",
    "Hint: make a plot of within cluster sum of square and then use the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1,20)\n",
    "costs = []\n",
    "for i in ks:\n",
    "    #print(i)\n",
    "    km = k_means(data, d=2, k=i, tol=1e-7, max_iter=500)\n",
    "    km.fit_model()\n",
    "    costs.append(km.get_cost())\n",
    "#plt.plot(ks,costs)\n",
    "#plt.show()\n",
    "plt.plot(ks[:10],costs[:10])\n",
    "plt.title('Cost for 1-10 clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pick K to be 3 because that is where the decrease in cost becomes smaller and where the elbow appears to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Segmentation Analysis <font color=\"red\">  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "The dataset contains the station location and many different weather features observed at each location.\n",
    "\t\t\n",
    "<h4 align = \"center\">\n",
    "Environment Canada    \n",
    "Monthly Values for July - 2015\t\n",
    "</h4>\n",
    "<html>\n",
    "<head>\n",
    "<style>\n",
    "table {\n",
    "    font-family: arial, sans-serif;\n",
    "    border-collapse: collapse;\n",
    "    width: 100%;\n",
    "}\n",
    "\n",
    "td, th {\n",
    "    border: 1px solid #dddddd;\n",
    "    text-align: left;\n",
    "    padding: 8px;\n",
    "}\n",
    "\n",
    "tr:nth-child(even) {\n",
    "    background-color: #dddddd;\n",
    "}\n",
    "</style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Name in the table</th>\n",
    "    <th>Meaning</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>Stn_Name</font></td>\n",
    "    <td><font color = \"green\"><strong>Station Name</font</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>Lat</font></td>\n",
    "    <td><font color = \"green\"><strong>Latitude (North+, degrees)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>Long</font></td>\n",
    "    <td><font color = \"green\"><strong>Longitude (West - , degrees)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Prov</td>\n",
    "    <td>Province</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Tm</td>\n",
    "    <td>Mean Temperature (°C)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwTm</td>\n",
    "    <td>Days without Valid Mean Temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>D</td>\n",
    "    <td>Mean Temperature difference from Normal (1981-2010) (°C)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"black\">Tx</font></td>\n",
    "    <td><font color = \"black\">Highest Monthly Maximum Temperature (°C)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwTx</td>\n",
    "    <td>Days without Valid Maximum Temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"black\">Tn</font></td>\n",
    "    <td><font color = \"black\">Lowest Monthly Minimum Temperature (°C)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwTn</td>\n",
    "    <td>Days without Valid Minimum Temperature</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>S</td>\n",
    "    <td>Snowfall (cm)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwS</td>\n",
    "    <td>Days without Valid Snowfall</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>S%N</td>\n",
    "    <td>Percent of Normal (1981-2010) Snowfall</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><font color = \"green\"><strong>P</font></td>\n",
    "    <td><font color = \"green\"><strong>Total Precipitation (mm)</font></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwP</td>\n",
    "    <td>Days without Valid Precipitation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>P%N</td>\n",
    "    <td>Percent of Normal (1981-2010) Precipitation</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>S_G</td>\n",
    "    <td>Snow on the ground at the end of the month (cm)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Pd</td>\n",
    "    <td>Number of days with Precipitation 1.0 mm or more</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BS</td>\n",
    "    <td>Bright Sunshine (hours)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>DwBS</td>\n",
    "    <td>Days without Valid Bright Sunshine</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>BS%</td>\n",
    "    <td>Percent of Normal (1981-2010) Bright Sunshine</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>HDD</td>\n",
    "    <td>Degree Days below 18 °C</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>CDD</td>\n",
    "    <td>Degree Days above 18 °C</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>Stn_No</td>\n",
    "    <td>Climate station identifier (first 3 digits indicate   drainage basin, last 4 characters are for sorting alphabetically).</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>NA</td>\n",
    "    <td>Not Available</td>\n",
    "  </tr>\n",
    "\n",
    "\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename='weather.csv'\n",
    "df = pd.read_csv(filename)\n",
    "df = df[pd.notnull(df[\"Tm\"])]\n",
    "df = df.reset_index(drop=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the data\n",
    "\n",
    "Read the code below carefully to learn how to plot location data (longitude and lattitude) on a map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (14,10)\n",
    "llon=-140\n",
    "ulon=-50\n",
    "llat=40\n",
    "ulat=65\n",
    "df = df[(df['Long'] > llon) & (df['Long'] < ulon) & (df['Lat'] > llat) &(df['Lat'] < ulat)]\n",
    "\n",
    "my_map = Basemap(projection='merc',\n",
    "            resolution = 'l', area_thresh = 1000.0,\n",
    "            llcrnrlon=llon, llcrnrlat=llat,\n",
    "            urcrnrlon=ulon, urcrnrlat=ulat) \n",
    "my_map.drawcoastlines()\n",
    "my_map.drawcountries()\n",
    "my_map.shadedrelief()\n",
    "\n",
    "\n",
    "## this is to change longitude and latitude to coordinates\n",
    "\n",
    "xs,ys = my_map(numpy.asarray(df.Long), numpy.asarray(df.Lat))\n",
    "df['xm']= xs.tolist()\n",
    "df['ym'] =ys.tolist()\n",
    "\n",
    "# plot the stations on the map\n",
    "for index,row in df.iterrows():\n",
    "    my_map.plot(row.xm, row.ym,markerfacecolor =([1,0,0]),  marker='o', markersize= 5, alpha = 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the following, you'll work on two datasets data1 (segmentation based on location data only) and data2 (segmentation based on location data as well as the temperature data) to perform k means methods with an appropriate k to do clustering and then label the clusters on two separate maps. \n",
    "\n",
    "- You need to justify every decision you make by appropriate plots or reasoning. For example, make a certain plot to explain your decision about which k to use.\n",
    "- You need to plot the stations on a map, and color them according to their clusters. Also, plot the center of each cluster and label them using their cluster numbers 0,1,...,k-1 as label; use a large enough fontsize (fontsize=25).\n",
    "- Print out the average temperature of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do not change anything in this block\n",
    "data1= df[['xm','ym']].to_numpy()\n",
    "data2 = df[['xm','ym','Tx','Tm','Tn']].to_numpy()\n",
    "\n",
    "data1 = numpy.nan_to_num(data1)\n",
    "data1 = StandardScaler().fit_transform(data1)\n",
    "data2 = numpy.nan_to_num(data2)\n",
    "data2 = StandardScaler().fit_transform(data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Elbow method to pick a k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1,15)\n",
    "costs = []\n",
    "for i in ks:\n",
    "    print(i)\n",
    "    km = k_means(data1, d=2, k=i, tol=1e-7, max_iter=500)\n",
    "    km.fit_model()\n",
    "    costs.append(km.get_cost())\n",
    "#plt.plot(ks,costs)\n",
    "#plt.show()\n",
    "plt.plot(ks[:10],costs[:10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pick K to be 3 because that is where the decrease in cost becomes smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a k_means model and make the plot on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1 = 3\n",
    "km = k_means(data1, d=2, k=k1, tol=1e-7, max_iter=500)\n",
    "km.fit_model()\n",
    "labels = km.get_labels()\n",
    "labels = labels.astype(int)\n",
    "\n",
    "rcParams['figure.figsize'] = (14,10)\n",
    "llon=-140\n",
    "ulon=-50\n",
    "llat=40\n",
    "ulat=65\n",
    "df = df[(df['Long'] > llon) & (df['Long'] < ulon) & (df['Lat'] > llat) &(df['Lat'] < ulat)]\n",
    "\n",
    "my_map = Basemap(projection='merc',\n",
    "            resolution = 'l', area_thresh = 1000.0,\n",
    "            llcrnrlon=llon, llcrnrlat=llat,\n",
    "            urcrnrlon=ulon, urcrnrlat=ulat)\n",
    "my_map.drawcoastlines()\n",
    "my_map.drawcountries()\n",
    "my_map.shadedrelief()\n",
    "\n",
    "\n",
    "## this is to change longitude and latitude to coordinates\n",
    "df['labels1'] = labels.tolist()\n",
    "\n",
    "colormap = ['r','m','b']\n",
    "# plot the stations on the map\n",
    "for index,row in df.iterrows():\n",
    "    my_map.plot(row.xm, row.ym, color=colormap[int(row.labels1)], marker='o', markersize= 5, alpha = 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print out the average temperature of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatures = np.zeros(k1)\n",
    "Tm = df['Tm'].to_numpy()\n",
    "for i in range(len(labels)):\n",
    "    temperatures[labels[i]] += Tm[i]\n",
    "temperatures = temperatures/len(labels)\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use Elbow method to pick a k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = np.arange(1,15)\n",
    "costs = []\n",
    "for i in ks:\n",
    "    print(i)\n",
    "    km = k_means(data2, d=5, k=i, tol=1e-7, max_iter=500)\n",
    "    km.fit_model()\n",
    "    costs.append(km.get_cost())\n",
    "plt.plot(ks,costs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pick K to be 5 because that is where the decrease in cost becomes smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Fit a k_means model and make the plot on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k2 = 5\n",
    "km = k_means(data1, d=2, k=k2, tol=1e-7, max_iter=500)\n",
    "km.fit_model()\n",
    "labels2 = km.get_labels()\n",
    "labels2 = labels2.astype(int)\n",
    "\n",
    "rcParams['figure.figsize'] = (14,10)\n",
    "llon=-140\n",
    "ulon=-50\n",
    "llat=40\n",
    "ulat=65\n",
    "df = df[(df['Long'] > llon) & (df['Long'] < ulon) & (df['Lat'] > llat) &(df['Lat'] < ulat)]\n",
    "\n",
    "my_map = Basemap(projection='merc',\n",
    "            resolution = 'l', area_thresh = 1000.0,\n",
    "            llcrnrlon=llon, llcrnrlat=llat,\n",
    "            urcrnrlon=ulon, urcrnrlat=ulat)\n",
    "my_map.drawcoastlines()\n",
    "my_map.drawcountries()\n",
    "my_map.shadedrelief()\n",
    "\n",
    "df['labels2'] = labels2.tolist()\n",
    "\n",
    "colormap = ['r', 'm', 'b', 'c', 'w']\n",
    "# plot the stations on the map\n",
    "for index,row in df.iterrows():\n",
    "    my_map.plot(row.xm, row.ym, color=colormap[int(row.labels2)], marker='o', markersize= 5, alpha = 0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Print out the average temperature of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tm = df['Tm'].to_numpy()\n",
    "temperatures = np.zeros(k2)\n",
    "for i in range(len(labels2)):\n",
    "    #print(Tm[i])\n",
    "    temperatures[labels2[i]] += Tm[i]\n",
    "temperatures = temperatures/len(labels2)\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of the cells). \n",
    "3. Select Cell -> Run All. This will run all the cells in order, and will take several minutes. <font color='red'>You will not get any grade if you don't follow this step strictly.</font>\n",
    "4. Once you've rerun everything, select File -> Download as -> PDF via LaTeX\n",
    "5. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
    "6. Submit your PDF on Gradescope.\n",
    "7. Pack all the relevant files (Project_2.ipynb) into a zipfile yourfirstname_yourlastname.zip and upload it to Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "142b",
   "language": "python",
   "name": "ex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
